{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data=np.load(open('./data/train_input.npy','rb'))\n",
    "train_label_data=np.load(open('./data/train_label.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/data_configs.json','r') as f :\n",
    "    prepro_config = json.load(f)\n",
    "    #print(prepro_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 나누기\n",
    "RNG_SEED=1234\n",
    "VALID_SPLIT=0.2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT,\n",
    "                                                   random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#파라미터 변수\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=10\n",
    "VOCAB_SIZE=prepro_config['vocab_size']\n",
    "EMB_SIZE=128\n",
    "\n",
    "def mapping_fn(X,Y=None):\n",
    "    inputs, labels = {'x':X}, Y\n",
    "    return inputs, labels\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label))\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성 신경망+맥스풀링\n",
    "#Conv1D 활용,총 3개의 합성곱 층, kernel_size=3,4,5\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    TRAIN = mode ==tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode ==tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT =mode ==tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer선언\n",
    "    embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x'])\n",
    "    \n",
    "    #embedding layer에 대한 output에 대해 dropout을 취함\n",
    "    dropout_emb = keras.layers.Dropout(rate=0.5)(embedding_layer)\n",
    "    \n",
    "    #filters=128, kernel_size=3,4,5 길이가 3,4,5인 128개의 다른 필터를 생성\n",
    "    #n-gram처럼 다양한 각도에서 문장을 봄\n",
    "    #conv1d는 (배치크기: 문장 숫자, 길이: 각 문장의 단어개수, 채널: 임베딩 출력 차원수)로 입력값을 받음\n",
    "    \n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool1 = keras.layers.GlobalMaxPool1D()(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool2 = keras.layers.GlobalMaxPool1D()(conv2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool3 = keras.layers.GlobalMaxPool1D()(conv3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([pool1, pool2, pool3]) #모아주기\n",
    "    \n",
    "    hidden=keras.layers.Dense(250, activation=tf.nn.relu)(concat)\n",
    "    dropout_hidden=keras.layers.Dropout(rate=0.5)(hidden)\n",
    "    logits = keras.layers.Dense(1,name='logits')(dropout_hidden)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    #학습, 검증, 평가의 단계로 나눔\n",
    "    \n",
    "    if PREDICT:\n",
    "        pred=tf.nn.sigmoid(logits)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions={'prob':tf.round(pred)})\n",
    "    \n",
    "    loss=tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "    \n",
    "    if EVAL:\n",
    "        pred=tf.nn.sigmoid(logits)\n",
    "        accuracy=tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc':accuracy})\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"./checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\PC\\\\자연어처리\\\\./checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F555223708>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#Estimator객체 생성\n",
    "cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-6-8e14c7d9c44c>:18: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt-16866\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 16866 into C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0278778e-09, step = 16867\n",
      "INFO:tensorflow:global_step/sec: 3.44369\n",
      "INFO:tensorflow:loss = 1.3733081e-09, step = 16967 (29.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.51407\n",
      "INFO:tensorflow:loss = 1.394163e-09, step = 17067 (28.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.39414\n",
      "INFO:tensorflow:loss = 2.179061e-09, step = 17167 (29.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.46929\n",
      "INFO:tensorflow:loss = 1.0238923e-09, step = 17267 (28.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.34004\n",
      "INFO:tensorflow:loss = 1.9786732e-09, step = 17367 (29.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.50087\n",
      "INFO:tensorflow:loss = 1.1291178e-09, step = 17467 (28.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.5744\n",
      "INFO:tensorflow:loss = 1.8435788e-09, step = 17567 (27.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.15417\n",
      "INFO:tensorflow:loss = 1.0991218e-09, step = 17667 (31.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.4175\n",
      "INFO:tensorflow:loss = 1.2840888e-09, step = 17767 (29.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.41205\n",
      "INFO:tensorflow:loss = 1.3004955e-09, step = 17867 (29.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.36373\n",
      "INFO:tensorflow:loss = 6.7906364e-10, step = 17967 (29.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.29503\n",
      "INFO:tensorflow:loss = 1.4262238e-09, step = 18067 (30.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.41367\n",
      "INFO:tensorflow:loss = 1.0835106e-09, step = 18167 (29.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.43666\n",
      "INFO:tensorflow:loss = 1.3956443e-09, step = 18267 (29.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.57559\n",
      "INFO:tensorflow:loss = 5.7492583e-10, step = 18367 (27.965 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18436 into C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 6.373152e-10.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1f555225748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_est.train(train_input_fn) #학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-04-24T04:10:55Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt-18436\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-04-24-04:10:58\n",
      "INFO:tensorflow:Saving dict for global step 18436: acc = 0.8806, global_step = 18436, loss = 1.1642536\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 18436: C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt-18436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8806, 'loss': 1.1642536, 'global_step': 18436}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_est.evaluate(eval_input_fn) #평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data=np.load(open('./data/test_input.npy','rb'))\n",
    "ids = np.load(open('./data/test_id.npy','rb'),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측데이터파이프라인\n",
    "def mapping_fn2(X):\n",
    "    inputs = {'x':X}\n",
    "    return inputs\n",
    "\n",
    "def test_input_fn2():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((test_input_data))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn2)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PC\\자연어처리\\./checkpoint/cnn/model.ckpt-18436\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "prediction=np.array([p['prob'] for p in cnn_est.predict(test_input_fn2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":list(ids),\"sentiment\":list(prediction)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./data/Bag_of_Words_model_test_cnn.csv',index=False,quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
